---
layout: portfolio
title: Software
published: true
---


<section id="content">

  <header>
    <h1>Software</h1>
    <p>Here&rsquo;s some of my standalone software projects that I think are pretty cool.</p>
  </header>

  <section id="pathtracer">
    <article>
      <div class="flextitle">
        <h2>Global illumination path tracer</h2>
        <a class="button" href="https://github.com/sdao/xray"><i class="fa fa-github"></i> GPU Version</a><a class="button" href="https://github.com/sdao/path-tracer"><i class="fa fa-github"></i> CPU Version</a>
      </div>
      <div class="reactive-container">
        <img src="/images/pathtracer_dragon_800iters_30min.png" alt="Cornell box and Stanford dragon, rendered with path tracing">
      </div>
      <p>
        A Monte Carlo global illumination path tracer written in C++, using either the <a href="https://developer.nvidia.com/optix">Nvidia OptiX engine</a> on a CUDA-capable GPU or the <a href="https://embree.github.io/">Intel Embree kernels</a> on a CPU.
        I implemented this path tracer while reading Pharr and Humphreys&rsquo; excellent book <em><a href="http://www.pbrt.org">Physically Based Rendering</a></em>.
        This Cornell box (with the Stanford dragon, of course) demonstrating different physically-accurate materials took 30 minutes to render.
      </p>
      <div class="reactive-container">
        <img src="/images/pathtracer_spheres_800iters_20min.png" alt="Row of spheres, demonstrating depth of field">
      </div>
      <p>
        This row of spheres demonstrates rendering with depth of field (the aperture being <em>&fnof;</em>/16 in this image). The rendering was stopped after about 20 minutes and 800 iterations. (All of these timings were recorded on a 2.3 GHz Intel Core i5.)
      </p>
      <h3>Features</h3>
      <ul>
        <li>Reads custom OBJ polygon meshes or Platonic solids</li>
        <li>BRDF-based diffuse (Lambert), specular (Phong), and metallic (dielectric) materials included</li>
        <li>Depth of field</li>
        <li>Multi-threaded</li>
        <li>Also samples direct illumination to reduce convergence time</li>
        <li>Can perform slightly-biased sampling to reduce noise</li>
        <li>JSON-based scene file format</li>
        <li>Outputs HDR imagery to OpenEXR format</li>
      </ul>
    </article>
  </section>

  <section id="reyes">
    <article>
      <div class="flextitle">
        <h2>Reyes renderer</h2>
        <a class="button" href="https://github.com/sdao/hugo-reyes"><i class="fa fa-github"></i> GitHub Repo</a>
      </div>
      <div class="reactive-container">
        <div class="video-container">
          <iframe src="//player.vimeo.com/video/88600315?title=0&amp;byline=0&amp;portrait=0" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
        </div>
      </div>
      <p>
        A parallelized micropolygon renderer in Scala.
        This renderer uses the <a href="http://en.wikipedia.org/wiki/Reyes_rendering">Reyes algorithm</a> popularized by Pixar&rsquo;s RenderMan, although falling out of use due to the advent of physically-plausible rendering. Supports BÃ©zier patches, displacement mapping, shadow maps, toon shading, and animation.
      </p>
    </article>
  </section>

  <section id="prompter">
    <article>
      <div class="flextitle">
        <h2>Web-based teleprompter</h2>
        <div>
          <a class="button download" href="/prompter"><i class="fa fa-play-circle"></i> Run Online</a><a class="button" href="https://github.com/sdao/prompter"><i class="fa fa-github"></i> GitHub Repo</a>
        </div>
      </div>
      <p>This is yet another online JavaScript teleprompter. I created this prompter to work specifically with the setup at Texas Student TV on the UT Austin campus, so your mileage may vary.</p>
    </article>
  </section>

</section>
